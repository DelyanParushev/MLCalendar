# ml/nlp_parser_ml.py
import re
import json
from datetime import datetime, timedelta, time, date
from typing import Optional, Tuple
import os
import requests

# Configuration for ML model loading
ENABLE_ML_MODEL = os.getenv("ENABLE_ML_MODEL", "true").lower() == "true"
USE_HF_SPACE = os.getenv("USE_HF_SPACE", "true").lower() == "true"
HF_SPACE_URL = os.getenv("HF_SPACE_URL", "https://dex7er999-calendar-nlp-api.hf.space")

print(f"ü§ñ ML Model enabled: {ENABLE_ML_MODEL}")
print(f"üöÄ Using HF Space: {USE_HF_SPACE}")
if USE_HF_SPACE:
    print(f"üåê HF Space URL: {HF_SPACE_URL}")

# Import ML libraries only if not using HF Space
if ENABLE_ML_MODEL and not USE_HF_SPACE:
    try:
        import torch
        from transformers import BertTokenizerFast, BertForTokenClassification
        ML_AVAILABLE = True
        print("ü§ñ ML libraries loaded successfully")
    except ImportError as e:
        print(f"‚ö†Ô∏è ML libraries not available: {e}")
        ML_AVAILABLE = False
else:
    if USE_HF_SPACE:
        print("üöÄ Using Hugging Face Space for ML inference")
        ML_AVAILABLE = True
    else:
        print("üö´ ML model loading disabled")
        ML_AVAILABLE = False

# Load model from Hugging Face Hub
MODEL_NAME = "dex7er999/NLPCalendar"

# Initialize model variables
tokenizer = None
model = None
LABELS = ["O", "B-TITLE", "I-TITLE", "B-TIME", "I-TIME", "B-DATE", "I-DATE", "B-DURATION", "I-DURATION"]

if ENABLE_ML_MODEL and ML_AVAILABLE:
    try:
        print(f"üì• Loading model from Hugging Face: {MODEL_NAME}")
        # Try to load the model from Hugging Face
        tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)
        model = BertForTokenClassification.from_pretrained(MODEL_NAME)
        model.eval()
        print(f"‚úÖ Successfully loaded model from Hugging Face: {MODEL_NAME}")
    except Exception as e:
        print(f"‚ùå Failed to load model from Hugging Face: {e}")
        ML_AVAILABLE = False

print(f"ü§ñ ML Model available: {ML_AVAILABLE}")

# –ö–∞—Ä—Ç–∏ –∑–∞ –¥–Ω–∏ –æ—Ç —Å–µ–¥–º–∏—Ü–∞—Ç–∞ (–Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏, lower-case)
# Python's datetime.weekday(): 0=Monday through 6=Sunday
WEEKDAYS = {
    "–ø–æ–Ω–µ–¥–µ–ª–Ω–∏–∫": 0,
    "–≤—Ç–æ—Ä–Ω–∏–∫": 1,
    "—Å—Ä—è–¥–∞": 2,
    "—á–µ—Ç–≤—ä—Ä—Ç—ä–∫": 3,
    "–ø–µ—Ç—ä–∫": 4,
    "—Å—ä–±–æ—Ç–∞": 5,
    "–Ω–µ–¥–µ–ª—è": 6,
    # Handle variations
    "—Å—ä–±–æ—Ç–∞.": 5,
    "–Ω–µ–¥–µ–ª—è.": 6,
    # Handle case variations
    "–ü–æ–Ω–µ–¥–µ–ª–Ω–∏–∫": 0,
    "–í—Ç–æ—Ä–Ω–∏–∫": 1,
    "–°—Ä—è–¥–∞": 2,
    "–ß–µ—Ç–≤—ä—Ä—Ç—ä–∫": 3,
    "–ü–µ—Ç—ä–∫": 4,
    "–°—ä–±–æ—Ç–∞": 5,
    "–ù–µ–¥–µ–ª—è": 6
}

# –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª–Ω–∏ –¥—É–º–∏
RELATIVE = {
    "–¥–Ω–µ—Å": 0, "—É—Ç—Ä–µ": 1, "–≤–¥—Ä—É–≥–∏–¥–µ–Ω": 2
}

# –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∑–∞ –≤—Ä–µ–º–µ –æ—Ç –¥–µ–Ω—è (–∞–∫–æ –Ω—è–º–∞ –∏–∑—Ä–∏—á–µ–Ω —á–∞—Å)
DAYTIME_HINTS = {
    "—Å—É—Ç—Ä–∏–Ω—Ç–∞": time(9, 0),
    "–æ–±–µ–¥": time(12, 0),
    "–Ω–∞–æ–±–µ–¥": time(12, 0),
    "—Å–ª–µ–¥–æ–±–µ–¥": time(15, 0),
    "–≤–µ—á–µ—Ä—Ç–∞": time(19, 0),
    "–≤–µ—á–µ—Ä": time(19, 0)
}

def _next_weekday(from_date: date, target_weekday: int) -> date:
    """–í—Ä—ä—â–∞ —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ –¥–∞—Ç–∞ –∑–∞ –¥–∞–¥–µ–Ω –¥–µ–Ω –æ—Ç —Å–µ–¥–º–∏—Ü–∞—Ç–∞ (>= —É—Ç—Ä–µ)."""
    # Convert datetime to date if needed
    current_date = from_date.date() if isinstance(from_date, datetime) else from_date
    
    # Get current weekday (0=Monday through 6=Sunday)
    current_weekday = current_date.weekday()
    
    # Calculate days until target
    days_ahead = target_weekday - current_weekday
    
    # If we have passed the target day this week or it's today,
    # we need to wait for next week
    if days_ahead <= 0:
        days_ahead += 7
    
    next_date = current_date + timedelta(days=days_ahead)
    
    # Verify we got the right weekday
    assert next_date.weekday() == target_weekday, f"Expected weekday {target_weekday}, got {next_date.weekday()}"
    
    return next_date

def _parse_day_from_tokens(day_tokens: list[str], now: datetime) -> Optional[date]:
    """–û–ø–∏—Ç–≤–∞ –¥–∞ –∏–∑–≤–ª–µ—á–µ –¥–∞—Ç–∞ (–¥–µ–Ω/–¥–∞—Ç–∞) –æ—Ç —Ç–æ–∫–µ–Ω–∏—Ç–µ —Å WHEN_DAY."""
    if not day_tokens:
        return None

    # Convert to lowercase and search in WEEKDAYS dictionary as is
    toks = day_tokens

    # 1) –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª–Ω–∏ –¥—É–º–∏ (–¥–Ω–µ—Å/—É—Ç—Ä–µ/–≤–¥—Ä—É–≥–∏–¥–µ–Ω)
    for t in toks:
        if t.lower() in RELATIVE:
            return (now + timedelta(days=RELATIVE[t.lower()])).date()

    # 2) –¥–µ–Ω –æ—Ç —Å–µ–¥–º–∏—Ü–∞—Ç–∞ - search with original case first, then try lowercase
    for t in toks:
        weekday_val = WEEKDAYS.get(t)  # Try exact match first
        if weekday_val is None:
            weekday_val = WEEKDAYS.get(t.lower())  # Try lowercase as fallback
        if weekday_val is not None:
            return _next_weekday(now, weekday_val)

    # 3) –¥–∞—Ç–∞ –æ—Ç —Ç–∏–ø–∞ "20—Ç–∏", "1–≤–∏", "2—Ä–∏", "7–º–∏", "20", "20."
    for t in toks:
        m = re.match(r"^(\d{1,2})(?:-?(–≤–∏|—Ä–∏|—Ç–∏|–º–∏))?\.?$", t)
        if m:
            day = int(m.group(1))
            y, mth = now.year, now.month
            try:
                candidate = date(y, mth, day)
            except ValueError:
                if mth == 12:
                    candidate = date(y + 1, 1, day)
                else:
                    candidate = date(y, mth + 1, day)
            if candidate < now.date():
                if candidate.month == 12:
                    candidate = date(candidate.year + 1, 1, candidate.day)
                else:
                    for i in range(1, 3):
                        try:
                            candidate = date(y + (mth + i - 1) // 12,
                                             ((mth + i - 1) % 12) + 1,
                                             day)
                            break
                        except ValueError:
                            continue
            return candidate

    return None

def _parse_time_from_tokens(time_tokens: list[str]) -> Tuple[Optional[time], Optional[time]]:
    """–û–ø–∏—Ç–≤–∞ –¥–∞ –∏–∑–≤–ª–µ—á–µ –Ω–∞—á–∞–ª–µ–Ω –∏ –∫—Ä–∞–µ–Ω —á–∞—Å –æ—Ç WHEN_START —Ç–æ–∫–µ–Ω–∏."""
    if not time_tokens:
        return None, None

    raw = " ".join(time_tokens).lower().strip()
    raw = raw.replace("—á.", "—á").replace("—á–∞—Å–∞", "—á").replace(" —á–∞—Å–∞", "—á").replace(" h", "—á")

    # –ù–∞–º–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ —á–∏—Å–ª–∞ –≤—ä–≤ —Ñ–æ—Ä–º–∞—Ç –ß–ß:–ú–ú –∏–ª–∏ –ß–ß
    times = []
    
    # –ü—ä—Ä–≤–æ —Ç—ä—Ä—Å–∏–º —Ñ–æ—Ä–º–∞—Ç –ß–ß:–ú–ú
    for match in re.finditer(r"\b(\d{1,2})[:\.](\d{1,2})\b", raw):
        h = int(match.group(1))
        m = int(match.group(2))
        if 0 <= h <= 23 and 0 <= m <= 59:
            times.append((time(h, m), match.start()))
    
    # –ü–æ—Å–ª–µ —Ç—ä—Ä—Å–∏–º —Ñ–æ—Ä–º–∞—Ç –ß–ß
    for match in re.finditer(r"\b(\d{1,2})\s*—á?\b", raw):
        # –ü—Ä–æ–ø—É—Å–∫–∞–º–µ, –∞–∫–æ –≤–µ—á–µ –∏–º–∞–º–µ —á–∏—Å–ª–æ —Å –º–∏–Ω—É—Ç–∏ –Ω–∞ —Ç–∞–∑–∏ –ø–æ–∑–∏—Ü–∏—è
        skip = False
        for _, pos in times:
            if pos <= match.start() <= pos + len(match.group(0)):
                skip = True
                break
        if skip:
            continue
            
        h = int(match.group(1))
        if 0 <= h <= 23:
            times.append((time(h, 0), match.start()))
    
    # –°–æ—Ä—Ç–∏—Ä–∞–º–µ –ø–æ –ø–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–∞
    times.sort(key=lambda x: x[1])
    times = [t for t, _ in times]  # –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ –ø–æ–∑–∏—Ü–∏–∏—Ç–µ
    
    # –ê–∫–æ –∏–º–∞–º–µ –¥–≤–µ –≤—Ä–µ–º–µ–Ω–∞ –∏ –≤—Ç–æ—Ä–æ—Ç–æ –µ —Å–ª–µ–¥ –¥—É–º–∞—Ç–∞ "–¥–æ", —Ç–æ–≤–∞ –µ –∫—Ä–∞–π–Ω–æ –≤—Ä–µ–º–µ
    if len(times) == 2 and "–¥–æ" in raw:
        return times[0], times[1]
    elif len(times) == 1:
        return times[0], None
    
    return None, None

def _find_daytime_hint(tokens: list[str], labels: list[str]) -> Optional[time]:
    """–ê–∫–æ –Ω—è–º–∞ WHEN_START, —Ç—ä—Ä—Å–∏–º –¥—É–º–∏ –∫–∞—Ç–æ '—Å—É—Ç—Ä–∏–Ω—Ç–∞', '—Å–ª–µ–¥–æ–±–µ–¥', '–≤–µ—á–µ—Ä—Ç–∞' –≤ WHEN_DAY —Ç–æ–∫–µ–Ω–∏."""
    for tok, lab in zip(tokens, labels):
        if lab in ("B-WHEN_DAY", "I-WHEN_DAY"):
            if tok.lower() in DAYTIME_HINTS:
                return DAYTIME_HINTS[tok.lower()]
    return None

def query_hf_space(text: str) -> dict:
    """Query the Hugging Face Space API for ML inference"""
    try:
        # Make API call to your HF Space
        response = requests.post(
            f"{HF_SPACE_URL}/api/parse",
            json={"text": text},
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        
        print(f"‚úÖ HF Space API call successful")
        return result
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå HF Space API error: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Unexpected error calling HF Space: {e}")
        return None

def parse_text(text: str) -> dict:
    if not text or not text.strip():
        return {"title": "", "datetime": None, "tokens": [], "labels": [], "debug": {"note": "empty text"}}
    
    # Try HF Space API first if enabled
    if USE_HF_SPACE and ML_AVAILABLE:
        print("üöÄ Using Hugging Face Space for parsing")
        hf_result = query_hf_space(text)
        if hf_result:
            return hf_result
        else:
            print("‚ö†Ô∏è HF Space failed, falling back to local processing")
    
    # Local ML model processing (if available)
    if not USE_HF_SPACE and ML_AVAILABLE and model is not None and tokenizer is not None:
        return parse_with_local_model(text)
    
    # Fallback parsing
    print("‚ö†Ô∏è Using simple fallback parsing")
    return parse_fallback(text)

def parse_fallback(text: str) -> dict:
    """Simple fallback parsing when ML model is not available"""
    words = text.split()
    now = datetime.now()
    
    # Look for common time patterns
    time_pattern = r'\b(\d{1,2})[:\.](\d{2})\b|\b(\d{1,2})\s*—á–∞—Å–∞?\b'
    time_matches = re.findall(time_pattern, text.lower())
    
    start_dt = None
    if time_matches:
        for match in time_matches:
            hour = int(match[0] or match[2])
            minute = int(match[1]) if match[1] else 0
            if 0 <= hour <= 23 and 0 <= minute <= 59:
                start_dt = datetime.combine(now.date(), time(hour, minute))
                if start_dt < now:
                    start_dt += timedelta(days=1)
                break
    
    return {
        "title": text.strip(),
        "datetime": start_dt,
        "start": start_dt,
        "end_datetime": None,
        "tokens": words,
        "labels": ["O"] * len(words),
        "debug": {"note": "fallback parsing - ML model not available"}
    }

def parse_with_local_model(text: str) -> dict:
    """Parse text using locally loaded ML model"""
        
        start_dt = None
        if time_matches:
            for match in time_matches:
                hour = int(match[0] or match[2])
                minute = int(match[1]) if match[1] else 0
                if 0 <= hour <= 23 and 0 <= minute <= 59:
                    start_dt = datetime.combine(now.date(), time(hour, minute))
                    if start_dt < now:
                        start_dt += timedelta(days=1)
                    break
        
        return {
            "title": text.strip(),
            "datetime": start_dt,
            "start": start_dt,
            "end_datetime": None,
            "tokens": words,
            "labels": ["O"] * len(words),
            "debug": {"note": "fallback parsing - ML model not available"}
        }

    words = text.split()
    encoding = tokenizer(words, is_split_into_words=True, return_tensors="pt", truncation=True, padding=True)

    with torch.no_grad():
        outputs = model(input_ids=encoding["input_ids"], attention_mask=encoding["attention_mask"])
    logits = outputs.logits
    pred_ids = torch.argmax(logits, dim=-1).squeeze().tolist()

    word_ids = encoding.word_ids(batch_index=0)
    labels = []
    current = None
    
    # First pass - get model predictions
    for idx, wid in enumerate(word_ids):
        if wid is None:
            continue
        if wid != current:
            current = wid
            label_id = pred_ids[idx]
            labels.append(LABELS[label_id])
    
    # Second pass - fix weekday labels if model missed them
    fixed_labels = []
    for word, label in zip(words, labels):
        if label == "O" and word.lower() in WEEKDAYS:
            # If it's a weekday but was labeled as Other, fix it
            fixed_labels.append("B-WHEN_DAY")
        else:
            fixed_labels.append(label)
    
    labels = fixed_labels

    tokens = words
    
    # –ù–∞–º–∏—Ä–∞–º–µ –∏–Ω–¥–µ–∫—Å–∏—Ç–µ –Ω–∞ –ø—ä—Ä–≤–∏—è –∏ –ø–æ—Å–ª–µ–¥–Ω–∏—è –∑–Ω–∞—á–∏–º —Ç–æ–∫–µ–Ω
    first_index = -1
    last_index = -1
    for i, (token, label) in enumerate(zip(tokens, labels)):
        # –°—ä–∑–¥–∞–≤–∞–º–µ —Å–ø–∏—Å—ä–∫ –æ—Ç –¥—É–º–∏, –∫–æ–∏—Ç–æ —Å–∞ —á–∞—Å—Ç –æ—Ç –∑–∞–≥–ª–∞–≤–∏–µ—Ç–æ
        title_words = ["–∫–ª–∞—Å", "–∫–æ–ª–µ–ª–µ—Ç–∞", "—Ç–∞—Ç–µ", "–æ—Ñ–∏—Å", "–º–æ–ª"]  # –¥–æ–±–∞–≤–µ—Ç–µ –æ—â–µ –¥—É–º–∏ –ø—Ä–∏ –Ω—É–∂–¥–∞
        
        if (label in ["B-TITLE", "B-PERSON", "B-PLACE"] or 
            token.lower() in title_words or 
            (i > 0 and tokens[i-1].lower() == "—Å" and token not in WEEKDAYS and not token.isdigit())):
            if first_index == -1:
                first_index = i
            last_index = i
    
    if first_index == -1:
        # –ê–∫–æ –Ω–µ —Å–º–µ –Ω–∞–º–µ—Ä–∏–ª–∏ –∑–Ω–∞—á–∏–º–∏ —Ç–æ–∫–µ–Ω–∏, –≤–∑–µ–º–∞–º–µ —Å–∞–º–æ B-TITLE
        title_tokens = [t for t, lab in zip(tokens, labels) if lab == "B-TITLE"]
        title = " ".join(title_tokens).strip()
    else:
        # –í–∑–µ–º–∞–º–µ –≤—Å–∏—á–∫–∏ —Ç–æ–∫–µ–Ω–∏ –º–µ–∂–¥—É –ø—ä—Ä–≤–∏—è –∏ –ø–æ—Å–ª–µ–¥–Ω–∏—è –∑–Ω–∞—á–∏–º —Ç–æ–∫–µ–Ω
        title_tokens = tokens[first_index:last_index + 1]
        
        # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ —Å–∞–º–æ –Ω–µ–∂–µ–ª–∞–Ω–∏—Ç–µ —Å–≤—ä—Ä–∑–≤–∞—â–∏ –¥—É–º–∏ –∏ –≤—Ä–µ–º–µ–≤–∏ –º–∞—Ä–∫–µ—Ä–∏
        filtered_tokens = []
        for i, token in enumerate(title_tokens):
            # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ –¥—É–º–∞—Ç–∞ –µ –≤—Ä–µ–º–µ–≤–∏ –º–∞—Ä–∫–µ—Ä –∏–ª–∏ –Ω–µ–∂–µ–ª–∞–Ω–∞ —Å–≤—ä—Ä–∑–≤–∞—â–∞ –¥—É–º–∞
            is_time_marker = token.lower() in ["—Å—É—Ç—Ä–∏–Ω—Ç–∞", "–≤–µ—á–µ—Ä—Ç–∞", "—Å–ª–µ–¥–æ–±–µ–¥", "—É—Ç—Ä–µ", "–¥–Ω–µ—Å", "–≤–¥—Ä—É–≥–∏–¥–µ–Ω"]
            is_weekday = token in WEEKDAYS or token.lower() in WEEKDAYS
            is_unwanted_connector = token.lower() in ["–Ω–∞", "–æ—Ç", "–¥–æ"]
            
            if not is_time_marker and not is_weekday and not is_unwanted_connector:
                filtered_tokens.append(token)
        
        title = " ".join(filtered_tokens).strip()

    day_tokens = [t for t, lab in zip(tokens, labels) if lab in ("B-WHEN_DAY", "I-WHEN_DAY")]
    
    # –°—ä–±–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ –≤—Ä–µ–º–µ–≤–∏ —Ç–æ–∫–µ–Ω–∏ –∑–∞–µ–¥–Ω–æ —Å—ä—Å —Å–≤—ä—Ä–∑–≤–∞—â–∏—Ç–µ –¥—É–º–∏ –º–µ–∂–¥—É —Ç—è—Ö
    time_section = []
    in_time_section = False
    for t, lab in zip(tokens, labels):
        if lab == "B-WHEN_START":
            in_time_section = True
            time_section.append(t)
        elif in_time_section and (lab == "O" or lab == "B-WHEN_START"):
            time_section.append(t)
        elif in_time_section:
            in_time_section = False
    
    start_tokens = time_section if time_section else []

    now = datetime.now()
    the_date = _parse_day_from_tokens(day_tokens, now)
    start_time, end_time = _parse_time_from_tokens(start_tokens) if start_tokens else (None, None)

    if start_time is None:
        start_time = _find_daytime_hint(tokens, labels)

    start_dt = None
    end_dt = None
    
    if the_date and start_time:
        start_dt = datetime.combine(the_date, start_time)
        if end_time:
            end_dt = datetime.combine(the_date, end_time)
            # If end time is earlier than start time, assume it's the next day
            if end_dt < start_dt:
                end_dt += timedelta(days=1)
    elif the_date and not start_time:
        start_dt = None
    elif not the_date and start_time:
        tentative = datetime.combine(now.date(), start_time)
        if tentative < now:
            tentative += timedelta(days=1)
        start_dt = tentative
        if end_time:
            end_dt = datetime.combine(tentative.date(), end_time)
            if end_dt < start_dt:
                end_dt += timedelta(days=1)

    if not title:
        non_when = [t for t, lab in zip(tokens, labels) if lab not in ("B-WHEN_DAY", "I-WHEN_DAY", "B-WHEN_START")]
        filtered = [t for t in non_when if t.lower() not in {"–Ω–∞", "–≤", "—Å", "–æ—Ç", "–¥–æ"}]
        title = " ".join(filtered).strip()

    return {
        "title": title,
        "datetime": start_dt,  # For backward compatibility
        "start": start_dt,
        "end_datetime": end_dt,
        "tokens": tokens,
        "labels": labels,
        "debug": {"note": "inference ok", "ml_enabled": ENABLE_ML_MODEL}
    }

if __name__ == "__main__":
    tests = [
        "–û–Ω–ª–∞–π–Ω –ª–µ–∫—Ü–∏—è –ø–æ –ø—Ä–æ–≥—Ä–∞–º–∏—Ä–∞–Ω–µ –≤ –ø–æ–Ω–µ–¥–µ–ª–Ω–∏–∫ –æ—Ç 10 –¥–æ 12",
        "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å —Ç–∞—Ç–µ —Å –∫–æ–ª–µ–ª–µ—Ç–∞ –≤ —Å—ä–±–æ—Ç–∞ –≤ 9",
        "–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å –¢–∞—Ç–µ —Å –ö–æ–ª–µ–ª–µ—Ç–∞ –≤ —Å—ä–±–æ—Ç–∞ –≤ 9",
        "–ô–æ–≥–∞ –∫–ª–∞—Å —É—Ç—Ä–µ —Å—É—Ç—Ä–∏–Ω—Ç–∞",
        "–ô–æ–≥–∞ –ö–ª–∞—Å —É—Ç—Ä–µ —Å—É—Ç—Ä–∏–Ω—Ç–∞",
        "–í–µ—á–µ—Ä—è —Å –≥–µ—Ä–∏ –≤ –Ω–µ–¥–µ–ª—è –æ—Ç 18",
        "–í–µ—á–µ—Ä—è —Å –ì–µ—Ä–∏ –≤ –ù–µ–¥–µ–ª—è –æ—Ç 18"
    ]
    
    print("\n–¢–µ—Å—Ç–≤–∞–Ω–µ –Ω–∞ –ø–∞—Ä—Å–µ—Ä–∞:")
    print("-" * 80)
    for t in tests:
        res = parse_text(t)
        print("\n–í—Ö–æ–¥:", t)
        print("–ó–∞–≥–ª–∞–≤–∏–µ:", res["title"])
        print("–¢–æ–∫–µ–Ω–∏:", ' | '.join(f"{t}[{l}]" for t, l in zip(res["tokens"], res["labels"])))
        start_tokens = [t for t, l in zip(res["tokens"], res["labels"]) if l == "B-WHEN_START"]
        print("–í—Ä–µ–º–µ–≤–∏ —Ç–æ–∫–µ–Ω–∏:", start_tokens)
        if start_tokens:
            start, end = _parse_time_from_tokens(start_tokens)
            print("–†–∞–∑–ø–æ–∑–Ω–∞—Ç–æ –≤—Ä–µ–º–µ - –Ω–∞—á–∞–ª–æ:", start, "–∫—Ä–∞–π:", end)
        print("–ù–∞—á–∞–ª–æ:", res["datetime"] or res["start"])
        print("–ö—Ä–∞–π:", res.get("end_datetime"))
        print("-" * 80)

    print("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞ –º–æ–¥–µ–ª–∞:")
    if ML_AVAILABLE and tokenizer and model:
        print("–ó–∞—Ä–µ–¥–µ–Ω –æ—Ç: Hugging Face Hub")
        print("–†–∞–∑–º–µ—Ä –Ω–∞ —Ä–µ—á–Ω–∏–∫–∞:", tokenizer.vocab_size)
        print("–ë—Ä–æ–π –µ—Ç–∏–∫–µ—Ç–∏:", model.config.num_labels)
        print("–ù–∞–ª–∏—á–Ω–∏ –µ—Ç–∏–∫–µ—Ç–∏:", LABELS)
    else:
        print("ML –º–æ–¥–µ–ª –Ω–µ –µ –∑–∞—Ä–µ–¥–µ–Ω - –∏–∑–ø–æ–ª–∑–≤–∞ —Å–µ fallback –ø–∞—Ä—Å–µ—Ä")